#!/usr/bin/env jython
#
#  Author: Hari Sekhon
#  Date: 2013-06-08 22:06:27 +0100 (Sat, 08 Jun 2013)
#
#  http://github.com/harisekhon
#
#  License: see accompanying LICENSE file
#

""" Jython program to find a slow Hadoop HDFS node by querying all the nodes for a given HDFS file and printing the read times """

# BASIC USAGE:  jython -J-cp `hadoop classpath` hadoop_hdfs_find_slow_node.jy -f /hdfs/path/to/file

# I tested this on one of my NN HA Kerberized clusters, should work fine under all circumstances where you have Jython and correctly deployed Hadoop client configuration as long as you have enough RAM, should be no more than 1GB or ~ java -Xmx + HDFS blocksize
#
# watch memory usage with top like so:    top -p $(pgrep -f jython|tr '\n' ','|sed 's/,$//')

__author__  = "Hari Sekhon"
__version__ = 0.2

# ============================================================================ #
#                               Example Usage
# ============================================================================ #
#
# 1. (Optional) Generate a test file. If you want to test all nodes, simply make sure the number of blocks
#    and replication factor is high enough that all nodes will contain blocks for this test file
#    This part is highly tunable depending on your circumstances and what you're trying to test, adjust to suit your needs if generating a workload,
#    ramp this up for a bigger cluster or just use one of the files you had problems with accessing slowly
#
#    >>>  dd if=/dev/urandom bs=10M count=100 | hadoop fs -D dfs.block.size=${BLOCK_SIZE:-$((10*1024*1024))} -D dfs.replication=${REPLICATION_FACTOR:-3} -put - /tmp/testfile
#
# 2. (Optional) inspect new test file's block locations
#
#    >>>  hadoop fsck /tmp/testfile -files -blocks -locations
#
# 3. Run this program against the file to see the block read speeds from the different datanodes
#
#    >>>  jython -J-cp `hadoop classpath` hadoop_hdfs_find_slow_node.jy
#
#    For prettier aligned output sorted by the slowest nodes at the top:
#
#    >>>  jython -J-cp `hadoop classpath` hadoop_hdfs_find_slow_node.jy | column -t | sort -k12nr
#
# ============================================================================ #

import os, sys, time
# Refusing to use either optparse or argparse since it's annoyingly non-portable across different versions of Python
import getopt
#import array # not use byte arrays any more
from java.nio import ByteBuffer
try:
    from org.apache.hadoop.conf import Configuration
    from org.apache.hadoop.fs import FileSystem
    from org.apache.hadoop.fs import Path
except ImportError:
    print "Couldn't find Hadoop Java classes, try adding -J-cp `hadoop classpath` to the jython arguments"
    sys.exit(2)
#from org.apache.hadoop.hdfs import DistributedFileSystem
#dfs = DistributedFileSystem()
#client = dfs.getClient()

def usage():
    """print usage and exit"""

    print >> sys.stderr, """
usage: %s -f /path/to/hdfs/file

-f --file               File in HDFS to read from all the datanodes containing the block
-o --one-block-per-DN   Only read 1 block from each DataNode for quickness, not as thorough. Optional, not the default
""" % os.path.basename(__file__)
    sys.exit(3)


def main():
    """parse cli args and call fetchFileBlocks to run the speed test"""

    try:
        opts, args = getopt.getopt(sys.argv[1:], "hf:o", ["help", "usage", "file=", "one-block-per-DN"])
    except getopt.GetoptError, e:
        print "error: %s" % e
        usage()
    filename = ""
    one_block_per_DN = False
    for o, a in opts:
        if o in ("-f", "--file"):
            filename = a
        elif o in ("-o", "--one-block-per-DN"):
            one_block_per_DN = True
        elif o in ("-h", "--help", "--usage"):
            usage()
        else:
            usage()
    if filename == "":
        usage()
    #if sys.argv[1]:
    #    filename = sys.argv[1]
    #else:
    #    usage()
    fetchFileBlocks(filename, one_block_per_DN)


def fetchFileBlocks(filename, one_block_per_DN):
    """Fetches all block replicas from all datanodes with timings"""

    conf = Configuration()
    path = Path(filename)
    fs   = FileSystem.get(conf)

    if not fs.exists(path):
        print "File not found: %s" % filename
        sys.exit(1)
    fh = fs.open(path)
    offset = 0
    length = 1
    block_num = 0
    highest_node_times = {}
    if(one_block_per_DN):
        datanodes_tested = []
    while True:
        block_num += 1
        block_locations = fs.getFileBlockLocations(path, offset, length)
        if not block_locations:
            break
        for block in block_locations:
            #print "block_locations " + block.toString()
            block_offset    = block.getOffset()
            block_length    = block.getLength()
            block_datanodes = block.getNames()
            #print "offset: %s, length: %s, datanodes: %s" % (block_offset, block_length, " ".join(block_datanodes))
            for datanode in block_datanodes:
                if(one_block_per_DN):
                    if(datanode in datanodes_tested):
                        continue
                    datanodes_tested.append(datanode)
                block.setNames([datanode])
                start_time = time.time()
                fh.seek(block_offset)
                fh.read(ByteBuffer.allocate(block_length))
                end_time = time.time()
                time_taken = end_time - start_time
                print "read block %d (offset %d, length %d) from datanode %s in %.4f secs" % (block_num, block_offset, block_length, datanode, time_taken)
                if(datanode in highest_node_times):
                    if(time_taken > highest_node_times[datanode]):
                        highest_node_times[datanode][time_taken] = [block_num, block_offset, block_length]
                else:
                    highest_node_times[datanode] = {time_taken: [block_num, block_offset, block_length]}
        offset = block_offset + block_length

    print "\nDataNodes by slowest first:\n"
    slowest_times = {}
    for datanode in highest_node_times.keys():
        slowest_time  = highest_node_times[datanode].keys()[0]
        block_num     = highest_node_times[datanode][slowest_time][0]
        block_offset  = highest_node_times[datanode][slowest_time][1]
        block_length  = highest_node_times[datanode][slowest_time][2]
        if(slowest_time in slowest_times):
            slowest_times[slowest_time].append([datanode, block_num, block_offset, block_length])
        else:
            slowest_times[slowest_time] = [[datanode, block_num, block_offset, block_length]]
    for time_taken in reversed(sorted(slowest_times.keys())):
        for t in slowest_times[time_taken]:
            print "datanode %s highest read time was for block %d (offset %d, length %d) => %.4f secs" % (t[0], t[1], t[2], t[3], time_taken)

    return slowest_times

if __name__ == "__main__":
    main()
