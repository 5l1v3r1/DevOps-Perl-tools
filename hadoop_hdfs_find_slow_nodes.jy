#!/usr/bin/env jython
#
#  Author: Hari Sekhon
#  Date: 2013-06-08 22:06:27 +0100 (Sat, 08 Jun 2013)
#
#  http://github.com/harisekhon
#
#  License: see accompanying LICENSE file
#
# vim:filetype=python

""" Jython program to find a slow Hadoop HDFS node by querying all the nodes for a given HDFS file and printing the read times """

# BASIC USAGE:  jython -J-cp `hadoop classpath` hadoop_hdfs_find_slow_node.jy -f /hdfs/path/to/file

# I tested this on one of my NN HA Kerberized clusters, should work fine under all circumstances where you have Jython and correctly deployed Hadoop client configuration as long as you have enough RAM, should be no more than 1GB or ~ java -Xmx + HDFS blocksize
#
# watch memory usage with top like so:    top -p $(pgrep -f jython|tr '\n' ','|sed 's/,$//')

__author__  = "Hari Sekhon"
__version__ = 0.5

# ============================================================================ #
#                               Example Usage
# ============================================================================ #
#
# 1. (Optional) Generate a test file. If you want to test all nodes, simply make sure the number of blocks
#    and replication factor is high enough that all nodes will contain blocks for this test file
#    This part is highly tunable depending on your circumstances and what you're trying to test, adjust to suit your needs if generating a workload,
#    ramp this up for a bigger cluster or just use one of the files you had problems with accessing slowly
#
#    >>>  dd if=/dev/urandom bs=10M count=100 | hadoop fs -D dfs.block.size=${BLOCK_SIZE:-$((10*1024*1024))} -D dfs.replication=${REPLICATION_FACTOR:-3} -put - /tmp/testfile
#
# 2. (Optional) inspect new test file's block locations
#
#    >>>  hadoop fsck /tmp/testfile -files -blocks -locations
#
# 3. Run this program against the file to see the block read speeds from the different datanodes
#
#    >>>  jython -J-cp `hadoop classpath` hadoop_hdfs_find_slow_node.jy -f /tmp/testfile
#
# ============================================================================ #

import os, sys, time, socket
# Refusing to use either optparse or argparse since it's annoyingly non-portable across different versions of Python
import getopt
#import array # not using byte arrays any more
from java.nio import ByteBuffer
try:
    from org.apache.hadoop.conf import Configuration
    #from org.apache.hadoop.fs import FileSystem
    from org.apache.hadoop.fs import Path
    from org.apache.hadoop.hdfs import DistributedFileSystem
except ImportError, e:
    print "Couldn't find Hadoop Java classes, try:  jython -J-cp `hadoop classpath` hadoop_hdfs_find_slow_nodes.jy <args>"
    sys.exit(2)

def usage():
    """print usage and exit"""

    print >> sys.stderr, """
usage: %s -f /path/to/hdfs/file

-f --file               File in HDFS to read from all blocks from to find out which datanode is the slowest from this client
-m --multiple-blocks    Fetch multiple copies not just the nearest copy to debug performance across nodes
-o --one-block-per-DN   Only read 1 block from each DataNode for quickness, not as thorough. Optional, not the default
""" % os.path.basename(__file__)
#-a --all-blocks         Fetch all copies of all blocks from all datanodes (--one-block-per-DN shortcuts this) [Not implemented yet]
# TODO: add multiple files and dir recursion
    sys.exit(3)


def main():
    """parse cli args and call fetchFileBlocks to run the speed test"""

    try:
        opts, args = getopt.getopt(sys.argv[1:], "hf:amo", ["help", "usage", "file=", "all-blocks", "multiple-blocks", "one-block-per-DN"])
    except getopt.GetoptError, e:
        print "error: %s" % e
        usage()
    filename         = ""
    all_blocks       = False
    multiple_blocks  = False
    one_block_per_DN = False
    for o, a in opts:
        if o in ("-f", "--file"):
            filename = a
        elif o in ("-a", "--all-blocks"):
            all_blocks = True
            print "--all not implemented yet"
            sys.exit(2)
        elif o in ("-m", "--multiple-blocks"):
            multiple_blocks = True
        elif o in ("-o", "--one-block-per-DN"):
            one_block_per_DN = True
        elif o in ("-h", "--help", "--usage"):
            usage()
        else:
            usage()
    if filename == "":
        usage()
    try:
        TestHDFSBlockReads(filename, multiple_blocks, one_block_per_DN).fetchFileBlocksTimed()
    except KeyboardInterrupt, e:
        print "Caught Control-C..."
        sys.exit(0)
    except IOError, e:
        print "%s" % e
        sys.exit(2)


class TestHDFSBlockReads:
    """Class to hold HDFS Block Read State"""

    def __init__(self, filename, multiple_blocks, one_block_per_DN):
        """Instantiate State"""

        self.multiple_blocks    = multiple_blocks
        self.one_block_per_DN   = one_block_per_DN
        self.datanodes_tested   = set()
        self.nodes_failed_reads = set()
        self.block_num          = 0
        self.offset             = 0
        self.length             = 1
        try:
            self.fqdn = socket.getfqdn()
            print ">> Running on %s" % self.fqdn
        except SocketError, e:
            print >> sys.stderr, "Failed to get fqdn of this local host, won't be able to tell you if we're reading from a local datanode\n\nSocketError: %s\n" % e
            self.fqdn = None

        conf      = Configuration()
        self.fs   = DistributedFileSystem.get(conf)
        self.path = Path(filename)

        if not self.fs.exists(self.path):
            raise IOError, "File not found: %s" % filename
        #self.fh = self.fs.open(path)
        # The client one tells you which DN you are reading from
        client = self.fs.getClient()
        #in = client.DFSDataInputStream(self.fh)
        self.fh = client.open(filename)
        if self.fh == None:
            raise IOError, "Failed to get client filehandle to HDFS file %s" % filename
        self.highest_node_times = {}

    def fetchFileBlocksTimed(self):
        """Fetches all block replicas from all datanodes with timings"""

        while True:
            self.block_num += 1
            block_locations = self.fs.getFileBlockLocations(self.path, self.offset, self.length)
            if not block_locations:
                break
            for block in block_locations:
                #print "block_locations " + block.toString()
                self.block_offset    = block.getOffset()
                self.block_length    = block.getLength()
                self.readBlockFromDNs()
                self.offset = self.block_offset + self.block_length

        print
        if self.fqdn:
            if self.fqdn in self.datanodes_tested:
                print "Local DataNode is %s\n" % self.fqdn
            else:
                print "No local DataNode reads\n"
        if self.nodes_failed_reads:
            print "The following nodes FAILED to return blocks:\n"
            for node in sorted(nodes_failed_reads):
                print node
            print"\n"
        print "Summary - DataNodes by highest block read time descending:\n"
        slowest_times = {}
        for datanode in self.highest_node_times.keys():
            slowest_time = self.highest_node_times[datanode].keys()[0]
            rack         = self.highest_node_times[datanode][slowest_time][0]
            block_num    = self.highest_node_times[datanode][slowest_time][1]
            block_offset = self.highest_node_times[datanode][slowest_time][2]
            block_length = self.highest_node_times[datanode][slowest_time][3]
            if(slowest_time in slowest_times):
                slowest_times[slowest_time].append([datanode, rack, block_num, block_offset, block_length])
            else:
                slowest_times[slowest_time] = [[datanode, rack, block_num, block_offset, block_length]]
        for time_taken in reversed(sorted(slowest_times.keys())):
            for t in slowest_times[time_taken]:
                print "datanode %s rack %s highest read time was for block %d (offset %d, length %d) => %.4f secs" % (t[0], t[1], t[2], t[3], t[4], time_taken)

        return slowest_times

    def readBlockFromDNs(self):
        """Read current block self.offset to self.length from all datanodes"""
        self.block_read_DNs = set()
        self.fh.seek(self.block_offset) # local node first otherwise NPE when trying seekToNewSource()
        # looks like this isn't populated until first read, check on second pass on seekToNewSource()
        self.dn = self.fh.getCurrentDatanode()
        self.readBlockFromDN()
        if self.multiple_blocks:
            while self.fh.seekToNewSource(self.block_offset):
                # self.dn must be populated by here or something is wrong
                self.dn = self.fh.getCurrentDatanode()
                if self.dn == None:
                    raise IOError, "Failed to get current DataNode for block %s offset %s length %s" % (self.block_num, self.block_offset, self.block_length)
                self.host = self.dn.getHostName()
                # I've come back to the first DN, done with this block
                if self.host in self.block_read_DNs:
                    return
                if not self.readBlockFromDN():
                    return

    def readBlockFromDN(self):
        """Read current block from current DN"""
        if self.dn:
            self.host = self.dn.getHostName()
            if(self.one_block_per_DN):
                if(self.host in self.datanodes_tested):
                    return False
            self.host = self.dn.getHostName()
            if(self.host in self.block_read_DNs):
                return False
        self.readBlockTimed()

    def readBlockTimed(self):
        """Read the current block with timings"""
        start_time = time.time()
        try:
            bytes = self.fh.read(ByteBuffer.allocate(self.block_length))
            if not bytes:
                self.dn    = self.fh.getCurrentDatanode()
                self.host  = self.dn.getHostName()
                self.nodes_failed_reads.add(self.host)
                print "Error: failed to read bytes from block %d (offset %d, length %d) from rack %s datanode %s: %s" % (self.block_num, self.block_offset, self.block_length, rack, self.host)
                return
        except Exception, e:
            self.dn    = self.fh.getCurrentDatanode()
            self.host  = self.dn.getHostName()
            self.nodes_failed_reads.add(self.host)
            print "Error: failed to read block %d (offset %d, length %d) from rack %s datanode %s: %s" % (self.block_num, self.block_offset, self.block_length, rack, self.host, e)
            return
        end_time   = time.time()
        time_taken = end_time - start_time
        self.dn    = self.fh.getCurrentDatanode()
        self.host  = self.dn.getHostName()
        rack       = self.dn.getNetworkLocation()
        self.block_read_DNs.add(self.host)
        self.datanodes_tested.add(self.host)
        isLocal = ""
        #if self.fqdn == self.host:
        #    isLocal = " (local)"
        print "read block %d (offset %d, length %d) from rack %s datanode %s in %.4f secs" % (self.block_num, self.block_offset, self.block_length, rack, self.host, time_taken)
        if(self.host in self.highest_node_times):
            if(time_taken > self.highest_node_times[self.host]):
                self.highest_node_times[self.host][time_taken] = [rack, self.block_num, self.block_offset, self.block_length]
        else:
            self.highest_node_times[self.host] = {time_taken: [rack, self.block_num, self.block_offset, self.block_length]}


if __name__ == "__main__":
    main()
